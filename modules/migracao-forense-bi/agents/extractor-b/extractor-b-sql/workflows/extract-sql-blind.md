# Workflow: [EXT-SQL-B] - Extra√ß√£o SQL CEGA (Blind Mode)

## Objetivo
Extrair **todas** as queries SQL, tabelas e cursores do arquivo `.lined` de forma **CEGA** (sem consultar Extractor-A-SQL) para garantir reconcilia√ß√£o anti-alucina√ß√£o v√°lida.

---

## üîí CR√çTICO: Modo Cego

Este workflow opera em **MODO CEGO**:
- ‚ùå **PROIBIDO** ler `claims_sql_A.json`
- ‚ùå **PROIBIDO** ler logs do Extractor-A-SQL
- ‚úÖ **OBRIGAT√ìRIO** extra√ß√£o 100% independente
- ‚úÖ **OBRIGAT√ìRIO** validar modo cego no in√≠cio e no fim

---

## Pr√©-requisitos

### 1. Verificar Arquivo .lined
```
Arquivo: run/extraction/{filename}.lined
Formato: LXXXX|conte√∫do
```

### 2. Verificar Knowledge Base
```
- knowledge/sql/sql-patterns-visualage.csv (30 padr√µes)
- knowledge/sql/sql-mapping-rules.csv (16 regras)
```

### 3. Verificar Modo Cego
```
- claims_sql_A.json N√ÉO deve ser acessado
- Extra√ß√£o deve ser 100% independente
```

---

## Etapas do Workflow

### Etapa 0: Verificar Modo Cego (CR√çTICO)

```python
print("üîí Verificando Modo Cego...")

# Lista de arquivos proibidos
forbidden_files = [
    "run/sql/extraction/claims_sql_A.json",
    "run/extraction/claims_A.json",
    "logs/extractor-a-sql.log",
    "logs/extractor-a.log"
]

# Verificar se algum arquivo proibido foi acessado
for forbidden_file in forbidden_files:
    if file_exists(forbidden_file):
        print(f"‚ö†Ô∏è Arquivo proibido existe: {forbidden_file}")
        print(f"   Garantindo que N√ÉO ser√° acessado...")
        
        # Marcar como proibido
        mark_as_forbidden(forbidden_file)

print("‚úÖ Modo Cego: ATIVO")
print("üîí Nenhum arquivo proibido ser√° acessado")
print("‚úÖ Extra√ß√£o ser√° 100% independente")
```

---

### Etapa 1: Carregar Arquivo .lined

```python
print("\nüîÑ Carregando arquivo .lined...")

# Carregar arquivo
lined_path = f"run/extraction/{filename}.lined"
lined_content = load_file(lined_path)

# Validar formato
if not lined_content:
    print("‚ùå Arquivo .lined vazio")
    exit(1)

# Converter para dicion√°rio
lined_dict = {}
for line in lined_content.split("\n"):
    if "|" in line:
        line_num_str, content = line.split("|", 1)
        line_num = int(line_num_str[1:])  # Remove 'L'
        lined_dict[line_num] = content

print(f"‚úÖ Arquivo carregado: {len(lined_dict)} linhas")
```

---

### Etapa 2: Carregar Padr√µes SQL

```python
print("üîÑ Carregando padr√µes SQL...")

# Carregar CSV
sql_patterns = load_csv("knowledge/sql/sql-patterns-visualage.csv")

# Ordenar por prioridade (HIGH ‚Üí MEDIUM ‚Üí LOW)
priority_order = {"HIGH": 3, "MEDIUM": 2, "LOW": 1}
sql_patterns.sort(key=lambda x: priority_order.get(x["PRIORITY"], 0), reverse=True)

# Compilar regex
compiled_patterns = []
for pattern in sql_patterns:
    try:
        regex = re.compile(pattern["REGEX_PATTERN"], re.IGNORECASE | re.DOTALL)
        compiled_patterns.append({
            "id": pattern["PATTERN_ID"],
            "regex": regex,
            "description": pattern["DESCRIPTION"],
            "priority": pattern["PRIORITY"]
        })
    except Exception as e:
        print(f"‚ö†Ô∏è Erro ao compilar padr√£o {pattern['PATTERN_ID']}: {e}")

print(f"‚úÖ Padr√µes carregados: {len(compiled_patterns)}")
```

---

### Etapa 3: Identificar Blocos EXEC SQL

```python
print("üîÑ Identificando blocos EXEC SQL...")

# Juntar linhas para an√°lise
full_content = "\n".join([f"L{num:04d}|{content}" for num, content in sorted(lined_dict.items())])

# Identificar blocos SQL
sql_blocks = []

for pattern in compiled_patterns:
    matches = pattern["regex"].finditer(full_content)
    
    for match in matches:
        # Extrair bloco SQL
        sql_text = match.group(0)
        
        # Calcular linhas
        text_before = full_content[:match.start()]
        line_start = text_before.count("\n") + 1
        line_end = line_start + sql_text.count("\n")
        
        # Evitar duplicatas
        if not any(b["line_start"] == line_start for b in sql_blocks):
            sql_blocks.append({
                "pattern_id": pattern["id"],
                "sql_text": sql_text,
                "line_start": line_start,
                "line_end": line_end,
                "description": pattern["description"]
            })

print(f"‚úÖ Blocos SQL identificados: {len(sql_blocks)}")
```

---

### Etapa 4: Extrair Queries (com Query IDs B)

```python
print("üîÑ Extraindo queries...")

queries = []
query_id_counter = 1

for block in sql_blocks:
    # Limpar SQL (remover LXXXX|)
    sql_lines = []
    for line in block["sql_text"].split("\n"):
        if "|" in line:
            sql_lines.append(line.split("|", 1)[1])
        else:
            sql_lines.append(line)
    
    sql_clean = "\n".join(sql_lines).strip()
    
    # Classificar tipo de query
    sql_upper = sql_clean.upper()
    if "PREPARE" in sql_upper or "EXECUTE" in sql_upper:
        query_type = "DYNAMIC"
    elif "DECLARE" in sql_upper and "CURSOR" in sql_upper:
        query_type = "CURSOR"
    else:
        query_type = "STATIC"
    
    # Classificar opera√ß√£o (CRUD)
    if "SELECT" in sql_upper or "FETCH" in sql_upper:
        operation_type = "READ"
    elif "INSERT" in sql_upper:
        operation_type = "CREATE"
    elif "UPDATE" in sql_upper:
        operation_type = "UPDATE"
    elif "DELETE" in sql_upper:
        operation_type = "DELETE"
    else:
        operation_type = "UNKNOWN"
    
    # Detectar tabelas
    affected_tables = []
    
    # FROM
    from_match = re.search(r'FROM\s+(\w+)', sql_upper)
    if from_match:
        affected_tables.append(from_match.group(1))
    
    # JOIN
    join_matches = re.findall(r'JOIN\s+(\w+)', sql_upper)
    affected_tables.extend(join_matches)
    
    # INTO
    into_match = re.search(r'INTO\s+(\w+)', sql_upper)
    if into_match:
        affected_tables.append(into_match.group(1))
    
    # UPDATE
    update_match = re.search(r'UPDATE\s+(\w+)', sql_upper)
    if update_match:
        affected_tables.append(update_match.group(1))
    
    # DELETE FROM
    delete_match = re.search(r'DELETE\s+FROM\s+(\w+)', sql_upper)
    if delete_match:
        affected_tables.append(delete_match.group(1))
    
    # Remover duplicatas
    affected_tables = list(set(affected_tables))
    
    # Calcular risco
    if "PREPARE" in sql_upper or "EXECUTE" in sql_upper:
        risk_level = "HIGH"
    elif operation_type == "DELETE" and "WHERE" not in sql_upper:
        risk_level = "HIGH"
    elif operation_type == "UPDATE" and "WHERE" not in sql_upper:
        risk_level = "HIGH"
    elif sql_upper.count("JOIN") >= 5:
        risk_level = "HIGH"
    elif "CURSOR" in sql_upper:
        risk_level = "MEDIUM"
    elif sql_upper.count("SELECT") > 1:
        risk_level = "MEDIUM"
    elif sql_upper.count("JOIN") >= 3:
        risk_level = "MEDIUM"
    else:
        risk_level = "LOW"
    
    # Gerar notas
    notes = []
    if "PREPARE" in sql_upper or "EXECUTE" in sql_upper:
        notes.append("SQL din√¢mico - dif√≠cil rastrear")
    if operation_type == "DELETE" and "WHERE" not in sql_upper:
        notes.append("DELETE sem WHERE - risco mass delete")
    if operation_type == "UPDATE" and "WHERE" not in sql_upper:
        notes.append("UPDATE sem WHERE - risco mass update")
    if sql_upper.count("JOIN") >= 5:
        notes.append(f"Query complexa - {sql_upper.count('JOIN')} JOINs")
    if "CURSOR" in sql_upper:
        notes.append("Cursor - considerar otimiza√ß√£o")
    if not notes:
        notes.append("Query simples")
    
    # Gerar evidence_pointer
    evidence_pointer = f"{filename}.esf:L{block['line_start']:04d}-L{block['line_end']:04d}"
    
    # Montar query (com Query ID B!)
    query = {
        "query_id": f"QRY-SQL-B-{query_id_counter:03d}",  # B, n√£o A!
        "query_type": query_type,
        "operation_type": operation_type,
        "sql_statement": sql_clean,
        "affected_tables": affected_tables,
        "evidence_pointer": evidence_pointer,
        "line_start": block["line_start"],
        "line_end": block["line_end"],
        "risk_level": risk_level,
        "notes": "; ".join(notes)
    }
    
    queries.append(query)
    query_id_counter += 1

print(f"‚úÖ Queries extra√≠das: {len(queries)}")
```

---

### Etapa 5: Identificar Tabelas

```python
print("üîÑ Identificando tabelas...")

tables = {}

for query in queries:
    for table_name in query["affected_tables"]:
        if table_name not in tables:
            tables[table_name] = {
                "table_name": table_name,
                "declaration_type": "EXEC SQL",
                "evidence_pointer": query["evidence_pointer"],
                "columns": [],
                "operations": []
            }
        
        # Adicionar opera√ß√£o
        if query["operation_type"] not in tables[table_name]["operations"]:
            tables[table_name]["operations"].append(query["operation_type"])

tables_list = list(tables.values())

print(f"‚úÖ Tabelas identificadas: {len(tables_list)}")
```

---

### Etapa 6: Analisar Cursores

```python
print("üîÑ Analisando cursores...")

cursors = []

for block in sql_blocks:
    sql_upper = block["sql_text"].upper()
    
    if "DECLARE" in sql_upper and "CURSOR" in sql_upper:
        # Extrair nome do cursor
        cursor_match = re.search(r'DECLARE\s+(\w+)\s+CURSOR', sql_upper)
        if cursor_match:
            cursor_name = cursor_match.group(1)
            
            # Extrair query do cursor
            query_match = re.search(r'FOR\s+(SELECT.*?)(?:END-EXEC|$)', sql_upper, re.DOTALL)
            cursor_query = query_match.group(1).strip() if query_match else ""
            
            # Detectar tabelas
            affected_tables = []
            from_match = re.search(r'FROM\s+(\w+)', cursor_query)
            if from_match:
                affected_tables.append(from_match.group(1))
            
            # Contar FETCHs
            fetch_count = full_content.upper().count(f"FETCH {cursor_name}")
            
            # Evidence pointer
            evidence_pointer = f"{filename}.esf:L{block['line_start']:04d}-L{block['line_end']:04d}"
            
            cursor = {
                "cursor_name": cursor_name,
                "cursor_query": cursor_query,
                "affected_tables": affected_tables,
                "evidence_pointer": evidence_pointer,
                "fetch_count": fetch_count
            }
            
            cursors.append(cursor)

print(f"‚úÖ Cursores analisados: {len(cursors)}")
```

---

### Etapa 7: Gerar JSON (com Metadata BLIND)

```python
print("üîÑ Gerando JSON...")

claims_sql_b = {
    "metadata": {
        "source_file": f"{filename}.esf",
        "extraction_date": datetime.now().isoformat(),
        "extractor_agent": "extractor-b-sql",
        "extraction_mode": "BLIND",  # CR√çTICO!
        "total_queries": len(queries),
        "total_tables": len(tables_list),
        "total_cursors": len(cursors)
    },
    "queries": queries,
    "tables": tables_list,
    "cursors": cursors
}

print("‚úÖ JSON gerado")
```

---

### Etapa 8: Validar Qualidade

```python
print("üîÑ Validando qualidade...")

errors = []

for query in queries:
    # Evidence pointer
    if not query.get("evidence_pointer"):
        errors.append(f"Query {query['query_id']} sem evidence_pointer")
    
    # Operation type
    if query.get("operation_type") not in ["READ", "CREATE", "UPDATE", "DELETE", "UNKNOWN"]:
        errors.append(f"Query {query['query_id']} com operation_type inv√°lido")
    
    # Affected tables (exceto SQL din√¢mico)
    if not query.get("affected_tables") and query.get("query_type") != "DYNAMIC":
        errors.append(f"Query {query['query_id']} sem affected_tables")
    
    # Risk level
    if query.get("risk_level") not in ["HIGH", "MEDIUM", "LOW"]:
        errors.append(f"Query {query['query_id']} com risk_level inv√°lido")
    
    # Query ID deve ser B, n√£o A
    if not query.get("query_id", "").startswith("QRY-SQL-B-"):
        errors.append(f"Query {query['query_id']} deve come√ßar com QRY-SQL-B-")

if errors:
    print("‚ùå Erros de valida√ß√£o:")
    for error in errors:
        print(f"   - {error}")
    exit(1)

print("‚úÖ Valida√ß√£o OK")
```

---

### Etapa 9: Salvar Output

```python
print("üîÑ Salvando output...")

output_path = "run/sql/extraction/claims_sql_B.json"
save_json(output_path, claims_sql_b)

print(f"‚úÖ Output salvo: {output_path}")
```

---

### Etapa 10: Validar Modo Cego (CR√çTICO)

```python
print("\nüîí Validando Modo Cego...")

# Verificar se algum arquivo proibido foi acessado
violations = []

for forbidden_file in forbidden_files:
    if file_was_accessed(forbidden_file):
        violations.append(forbidden_file)

if violations:
    print("‚ùå VIOLA√á√ÉO DE MODO CEGO!")
    print("   Arquivos proibidos foram acessados:")
    for violation in violations:
        print(f"   - {violation}")
    print("\n   Consequ√™ncias:")
    print("   - Extra√ß√£o N√ÉO √© independente")
    print("   - Reconcilia√ß√£o ser√° INV√ÅLIDA")
    print("   - Anti-alucina√ß√£o comprometida")
    print("\n   A√á√ÉO REQUERIDA:")
    print("   - Refazer extra√ß√£o sem acessar arquivos proibidos")
    exit(1)

print("‚úÖ Modo Cego: MANTIDO")
print("‚úÖ Nenhum arquivo proibido foi acessado")
print("‚úÖ Extra√ß√£o 100% independente")
print("‚úÖ Reconcilia√ß√£o ser√° v√°lida")
```

---

### Etapa 11: Exibir Estat√≠sticas

```python
print("\n" + "="*60)
print("‚úÖ EXTRA√á√ÉO SQL CEGA CONCLU√çDA")
print("="*60)

print(f"\nüîí Modo Cego: ATIVO")
print(f"   - Nenhum arquivo proibido foi acessado")
print(f"   - Extra√ß√£o 100% independente")
print(f"   - Reconcilia√ß√£o ser√° v√°lida")

print(f"\nüìä Estat√≠sticas:")
print(f"   - Queries: {len(queries)}")
print(f"   - Tabelas: {len(tables_list)}")
print(f"   - Cursores: {len(cursors)}")

# Contar por opera√ß√£o
read_count = sum(1 for q in queries if q["operation_type"] == "READ")
create_count = sum(1 for q in queries if q["operation_type"] == "CREATE")
update_count = sum(1 for q in queries if q["operation_type"] == "UPDATE")
delete_count = sum(1 for q in queries if q["operation_type"] == "DELETE")

print(f"\nüìà Opera√ß√µes:")
print(f"   - READ: {read_count}")
print(f"   - CREATE: {create_count}")
print(f"   - UPDATE: {update_count}")
print(f"   - DELETE: {delete_count}")

# Contar por risco
high_risk = sum(1 for q in queries if q["risk_level"] == "HIGH")
medium_risk = sum(1 for q in queries if q["risk_level"] == "MEDIUM")
low_risk = sum(1 for q in queries if q["risk_level"] == "LOW")

print(f"\n‚ö†Ô∏è Riscos:")
print(f"   - HIGH: {high_risk}")
print(f"   - MEDIUM: {medium_risk}")
print(f"   - LOW: {low_risk}")

print(f"\nüìÅ Output: {output_path}")
print("\n‚úÖ Extra√ß√£o SQL CEGA conclu√≠da com sucesso!")
print("‚úÖ Pronto para reconcilia√ß√£o com claims_sql_A.json")
```

---

## Output

**Arquivo**: `run/sql/extraction/claims_sql_B.json`

**Estrutura**:
```json
{
  "metadata": {
    "extraction_mode": "BLIND",
    ...
  },
  "queries": [...],
  "tables": [...],
  "cursors": [...]
}
```

---

## Valida√ß√£o

Verificar:
- ‚úÖ Todas as queries t√™m `evidence_pointer`
- ‚úÖ Todas as queries t√™m `operation_type` v√°lido
- ‚úÖ Todas as queries t√™m `affected_tables` (exceto DYNAMIC)
- ‚úÖ Todas as queries t√™m `risk_level` v√°lido
- ‚úÖ Query IDs s√£o `QRY-SQL-B-XXX` (n√£o `QRY-SQL-A-XXX`)
- ‚úÖ Metadata indica `extraction_mode: "BLIND"`
- ‚úÖ **Modo Cego foi mantido**
- ‚úÖ **Nenhum arquivo proibido foi acessado**
- ‚úÖ **Extra√ß√£o 100% independente**

---

**Status**: ‚úÖ Workflow Completo  
**Vers√£o**: 1.0  
**Data**: 2025-12-28  
**Modo**: BLIND (Extra√ß√£o Cega)



